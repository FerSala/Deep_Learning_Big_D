{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset MNIST - Yann LeCun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar MNIST\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X = mnist.data\n",
    "y = mnist.target.astype(int)\n",
    "\n",
    "# Mostrar 10 ejemplos\n",
    "plt.figure(figsize=(10, 2))\n",
    "for i in range(10):\n",
    "    image = X[i].reshape(28, 28)\n",
    "    label = y[i]\n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.title(str(label))\n",
    "    plt.axis(\"off\")\n",
    "plt.suptitle(\"Primeras 10 imágenes de MNIST\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Número de imágenes:\", X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_layers, output_size, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layers = [input_size] + hidden_layers + [output_size]\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.loss_history = []\n",
    "        total_params = 0\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            w = np.random.randn(self.layers[i], self.layers[i+1]) * np.sqrt(1. / self.layers[i])\n",
    "            b = np.zeros((1, self.layers[i+1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "            # Count parameters\n",
    "            num_params = self.layers[i] * self.layers[i+1] + self.layers[i+1]\n",
    "            print(f\"  Capa {i+1}: ({self.layers[i]} x {self.layers[i+1]}) | Parámetros: {num_params}\")\n",
    "            total_params += num_params\n",
    "        print(f\"\\nTotal de parámetros entrenables: {total_params}\\n\")\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_derivative(self, a):\n",
    "        return a * (1 - a)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def cross_entropy_loss(self, y_pred, y_true):\n",
    "        m = y_true.shape[0]\n",
    "        loss = -np.sum(y_true * np.log(y_pred + 1e-9)) / m\n",
    "        return loss\n",
    "\n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        zs = []\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            z = np.dot(activations[-1], w) + b\n",
    "            zs.append(z)\n",
    "            a = self.sigmoid(z)\n",
    "            activations.append(a)\n",
    "\n",
    "        # Output layer with softmax\n",
    "        z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        zs.append(z)\n",
    "        a = self.softmax(z)\n",
    "        activations.append(a)\n",
    "        return activations, zs\n",
    "\n",
    "    def backward(self, X, y, activations, zs):\n",
    "        grads_w = [0] * len(self.weights)\n",
    "        grads_b = [0] * len(self.biases)\n",
    "\n",
    "        # Output layer error\n",
    "        delta = activations[-1] - y\n",
    "        grads_w[-1] = np.dot(activations[-2].T, delta)\n",
    "        grads_b[-1] = np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "        # Backpropagate through hidden layers\n",
    "        for l in range(len(self.layers) - 2, 0, -1):\n",
    "            delta = np.dot(delta, self.weights[l].T) * self.sigmoid_derivative(activations[l])\n",
    "            grads_w[l-1] = np.dot(activations[l-1].T, delta)\n",
    "            grads_b[l-1] = np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "        return grads_w, grads_b\n",
    "\n",
    "    def update_params(self, grads_w, grads_b, batch_size):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * grads_w[i] / batch_size\n",
    "            self.biases[i] -= self.learning_rate * grads_b[i] / batch_size\n",
    "\n",
    "    def train(self, X, y, epochs=100, batch_size=32):\n",
    "        self.loss_history = []\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data\n",
    "            indices = np.arange(X.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            X = X[indices]\n",
    "            y = y[indices]\n",
    "\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X[i:i+batch_size]\n",
    "                y_batch = y[i:i+batch_size]\n",
    "\n",
    "                activations, zs = self.forward(X_batch)\n",
    "                grads_w, grads_b = self.backward(X_batch, y_batch, activations, zs)\n",
    "                self.update_params(grads_w, grads_b, X_batch.shape[0])\n",
    "\n",
    "            # Loss after epoch\n",
    "            y_pred = self.forward(X)[0][-1]\n",
    "            loss = self.cross_entropy_loss(y_pred, y)\n",
    "            self.loss_history.append(loss)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        output = self.forward(X)[0][-1]\n",
    "        return np.argmax(output, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar MNIST\n",
    "print(\"Cargando MNIST...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(mnist.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist.data / 255.0  # Normalizamos los píxeles a [0, 1]\n",
    "y = mnist.target.astype(int).reshape(-1, 1)\n",
    "\n",
    "# One-hot encode de etiquetas\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "# División en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "print(\"Datos cargados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar la red\n",
    "nn = NeuralNetwork(input_size=784, hidden_layers=[64, 32], output_size=10, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar\n",
    "nn.train(X_train, y_train, epochs=50, batch_size=64)\n",
    "\n",
    "plt.plot(nn.loss_history, label='Loss')\n",
    "plt.xlabel(\"Épocas\")\n",
    "plt.ylabel(\"Pérdida (Loss)\")\n",
    "plt.title(\"Evolución del Loss durante el entrenamiento\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar en test\n",
    "predictions = nn.predict(X_test)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = np.mean(predictions == y_test_labels)\n",
    "print(f\"Test accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular matriz de confusión\n",
    "cm = confusion_matrix(y_test_labels, predictions)\n",
    "\n",
    "# Mostrar matriz de confusión\n",
    "plt.figure(figsize=(8, 8))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(10))\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title(\"Matriz de Confusión - Clasificación MNIST\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy general\n",
    "acc = accuracy_score(predictions, y_test_labels)\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Métricas por clase\n",
    "precision = precision_score(predictions, y_test_labels, average=None)\n",
    "recall = recall_score(predictions, y_test_labels, average=None)\n",
    "f1 = f1_score(predictions, y_test_labels, average=None)\n",
    "\n",
    "# Imprimir todo por clase\n",
    "for i in range(10):\n",
    "    print(f\"Clase {i} → Precision: {precision[i]:.4f}, Recall: {recall[i]:.4f}, F1-score: {f1[i]:.4f}\")\n",
    "\n",
    "# Reporte completo (opcional)\n",
    "print(\"\\nReporte de clasificación completo:\")\n",
    "print(classification_report(predictions, y_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(ruta_img, nn):\n",
    "    # Cargar imagen, convertir a escala de grises y redimensionar\n",
    "    img = Image.open(ruta_img).convert(\"L\").resize((28, 28))\n",
    "    img_arr = np.array(img)\n",
    "\n",
    "    # Invertir colores si fondo blanco\n",
    "    if np.mean(img_arr) > 127:\n",
    "        img_arr = 255 - img_arr\n",
    "\n",
    "    # Normalizar y aplanar\n",
    "    img_norm = img_arr / 255.0\n",
    "    input_data = img_norm.reshape(1, -1)\n",
    "\n",
    "    # Predicción\n",
    "    pred = nn.predict(input_data)[0]\n",
    "\n",
    "    # Mostrar imagen y resultado\n",
    "    plt.imshow(img_arr, cmap=\"gray\")\n",
    "    plt.title(f\"Predicción: {pred}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(\"images/example.png\", nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
